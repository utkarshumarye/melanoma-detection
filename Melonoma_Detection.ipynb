{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Melonoma Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVKwd1MmFLzM"
      },
      "source": [
        "***Importing required Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_8w6IGrRnYU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv75laKHCosQ"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation,Flatten\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbt9oBxqFi3P"
      },
      "source": [
        "***Loading the dataset and applying the filters***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX1iWOLBCrbQ"
      },
      "source": [
        "def Dataset_loader(DIR, RESIZE, sigmaX=10):\n",
        "    IMG = []\n",
        "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
        "    for IMAGE_NAME in tqdm(os.listdir(DIR)):\n",
        "        PATH = os.path.join(DIR,IMAGE_NAME)\n",
        "        _, ftype = os.path.splitext(PATH)\n",
        "        if ftype == \".png\":\n",
        "            img = read(PATH)\n",
        "            gauss = cv2.GaussianBlur(img, (7,7), 3) # Gausian blur\n",
        "            unsharp_image = cv2.addWeighted(img, 3, gauss, -1.9, 0) #usharp marking\n",
        "\n",
        "            median_img = cv2.medianBlur(unsharp_image,5)\n",
        "            img = cv2.resize(median_img, (RESIZE,RESIZE))\n",
        "           \n",
        "            IMG.append(np.array(img))\n",
        "    return IMG\n",
        "    \n",
        "\n",
        "benign_train = np.array(Dataset_loader('/content/drive/MyDrive/Training/Benign',120))\n",
        "malign_train = np.array(Dataset_loader('/content/drive/MyDrive/Training/Malignant',120))\n",
        "benign_test = np.array(Dataset_loader('/content/drive/MyDrive/Testing/Benign',120))\n",
        "malign_test = np.array(Dataset_loader('/content/drive/MyDrive/Testing/Malignant',120))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSrk8hEAGajx"
      },
      "source": [
        "***Creating numpy array of zeros for labeling Benign images and numpy array of ones for labeling Mlaignant images. In the latter part the dataset is shuffled, labels are coverted into categorical format and the data is normalized.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVNdPNimCty3"
      },
      "source": [
        "img_size = 120\n",
        "\n",
        "# Create labels\n",
        "benign_train_label = np.zeros(len(benign_train))\n",
        "malign_train_label = np.ones(len(malign_train))\n",
        "benign_test_label = np.zeros(len(benign_test))\n",
        "malign_test_label = np.ones(len(malign_test))\n",
        "\n",
        "# Merge data \n",
        "x_train = np.concatenate((benign_train, malign_train), axis = 0)\n",
        "y_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)\n",
        "x_test = np.concatenate((benign_test, malign_test), axis = 0)\n",
        "y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\n",
        "\n",
        "# Shuffle train data\n",
        "s = np.arange(x_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "x_train = x_train[s]\n",
        "x_train = y_train[s]\n",
        "\n",
        "# Shuffle test data\n",
        "s = np.arange(x_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "x_test = x_test[s]\n",
        "y_test = y_test[s]\n",
        "\n",
        "# To categorical\n",
        "y_train = to_categorical(y_train, num_classes= 2)\n",
        "y_test = to_categorical(y_test, num_classes= 2)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = np.array(x_train) / 255\n",
        "x_test = np.array(x_test) / 255\n",
        "\n",
        "x_train.reshape(-1, img_size, img_size, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test.reshape(-1, img_size, img_size, 1)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmopZ3u-HZof"
      },
      "source": [
        "***In this step the dataset is splitted into two sets - train and test sets with 80% and 20% images respectively and images are displayed.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGgeq2HwCwgQ"
      },
      "source": [
        "w=60\n",
        "h=40\n",
        "fig=plt.figure(figsize=(15, 15))\n",
        "columns = 4\n",
        "rows = 3\n",
        "\n",
        "for i in range(1, columns*rows +1):\n",
        "    ax = fig.add_subplot(rows, columns, i)\n",
        "    if np.argmax(Y_train[i]) == 0:\n",
        "        ax.title.set_text('Benign')\n",
        "    else:\n",
        "        ax.title.set_text('Malignant')\n",
        "    plt.imshow(x_train[i], interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChCk8ajmXi_"
      },
      "source": [
        "***Checking for the number of Images in Training anf Testing.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kFEo2ezju9B"
      },
      "source": [
        "benign_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjt6kkxLj2K_"
      },
      "source": [
        "malign_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcfMRLbVj6KP"
      },
      "source": [
        "benign_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwDwS7p5j8O1"
      },
      "source": [
        "malign_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiIaGpEcIaDg"
      },
      "source": [
        "***Data Augumentation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D0uk8Z6ELQT"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_generator = ImageDataGenerator(\n",
        "        zoom_range=2,  # set range for random zoom\n",
        "        rotation_range = 90,\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYMTId6oIfZ-"
      },
      "source": [
        "***In this step we use CNN model to extract features and then the extracted features are fed to the other models.***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQOoB95CEL8E"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16,(5,5),padding='valid',input_shape = x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Conv2D(32,(5,5),padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Conv2D(64,(5,5),padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, name=\"dense_1\"))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model_feat = Model(inputs=model.input,outputs=model.get_layer('dense_1').output)\n",
        "feat_train = model_feat.predict(x_train) #Extracted Features from CNN\n",
        "feat_test = model_feat.predict(x_test)\n",
        "print(feat_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p40Yq2pcdimm"
      },
      "source": [
        "\n",
        " **1. Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-6htCADERlU"
      },
      "source": [
        "#using kernel = rbf\n",
        "from sklearn.svm import SVC\n",
        "svclassifier = SVC()\n",
        "svclassifier = SVC(kernel='rbf', random_state=42,C=1.0, degree=3, \n",
        "          gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
        "          max_iter=-1, decision_function_shape='ovr', break_ties=False)\n",
        "svclassifier.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "y_pred = svclassifier.predict(feat_test)\n",
        "\n",
        "#print(\"Accuracy of SVM (kernel = 'rbf') is :\",svm.score(feat_test,np.argmax(y_test,axis=1)))\n",
        "\n",
        "#from sklearn.metrics import classification_report, confusion_matrix\n",
        "#print(confusion_matrix(np.argmax(y_test,axis=1),y_pred))\n",
        "#print(classification_report(np.argmax(y_test,axis=1),y_pred))\n",
        "#recall_metric = recall_score((np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import precision_score\n",
        "print('Precision: %.3f' % precision_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import recall_score\n",
        "print('Recall: %.3f' % recall_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import f1_score\n",
        "print('F1 Score: %.3f' % f1_score(np.argmax(y_test,axis=1), y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeL0RxReETOV"
      },
      "source": [
        "#using kernel = linear\n",
        "\"\"\"from sklearn.svm import SVC\n",
        "svclassifier = SVC()\n",
        "svclassifier = SVC(kernel='linear',random_state=42,C=100,cache_size=200, class_weight=None, coef0=0.0,\n",
        "  decision_function_shape='ovr', degree=3, gamma='auto',\n",
        "  max_iter=-1, probability=True, shrinking=True,\n",
        "  tol=0.001, verbose=False)\n",
        "svm.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "\n",
        "print(\"Accuracy of SVM (kernel = 'linear') is :\",svm.score(feat_test,np.argmax(y_test,axis=1)))\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(np.argmax(y_test,axis=1),y_pred))\n",
        "print(classification_report(np.argmax(y_test,axis=1),y_pred))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZWiEjuHET_s"
      },
      "source": [
        "#using kernel = poly\n",
        "\"\"\"from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svm = SVC(kernel='poly',random_state=42,C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
        "  decision_function_shape='ovr', degree=3, gamma='auto',\n",
        "  max_iter=-1, probability=True, shrinking=True,\n",
        "  tol=0.001, verbose=False)\n",
        "svm.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "\n",
        "print(\"Accuracy of SVM (kernel = 'polynomial') is :\",svm.score(feat_test,np.argmax(y_test,axis=1)))\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(np.argmax(y_test,axis=1),y_pred))\n",
        "print(classification_report(np.argmax(y_test,axis=1),y_pred))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUBMaYhJEXQT",
        "outputId": "445cced5-03ef-48ce-bcba-512d17263250"
      },
      "source": [
        "#using kernel =sigmoid\n",
        "\"\"\"from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svm = SVC(kernel='sigmoid',random_state=42,C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
        "  decision_function_shape='ovr', degree=3, gamma='auto',\n",
        "  max_iter=-1, probability=True, shrinking=True,\n",
        "  tol=0.001, verbose=False)\n",
        "svm.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "\n",
        "print(\"Accuracy of SVM (kernel = 'sigmoid') is :\",svm.score(feat_test,np.argmax(y_test,axis=1)))\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of SVM (kernel = 'sigmoid') is : 0.7250280583613917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uTadj2ZJaQB"
      },
      "source": [
        "**2. K Nearest Neighbor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5emUIcCEZ8c"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='auto', leaf_size=30, p=5, metric='minkowski', metric_params=None, n_jobs=1)\n",
        "knn.fit(feat_train,np.argmax(y_train,axis=-1))\n",
        "y_pred=knn.predict(feat_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import precision_score\n",
        "print('Precision: %.3f' % precision_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import recall_score\n",
        "print('Recall: %.3f' % recall_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import f1_score\n",
        "print('F1 Score: %.3f' % f1_score(np.argmax(y_test,axis=1), y_pred))\n",
        "\n",
        "\n",
        "#print(\"The Accuracy of KNN is :\",knn.score(feat_test,np.argmax(y_test,axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY2FlDr8J8mn"
      },
      "source": [
        "**3. Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN3eZK92Edx8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(criterion= 'gini', min_samples_leaf= 3, min_samples_split= 7, n_estimators= 25, n_jobs= -1, random_state= 123)\n",
        "rfc.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "y_pred=rfc.predict(feat_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import precision_score\n",
        "print('Precision: %.3f' % precision_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import recall_score\n",
        "print('Recall: %.3f' % recall_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import f1_score\n",
        "print('F1 Score: %.3f' % f1_score(np.argmax(y_test,axis=1), y_pred))\n",
        "\n",
        "#print(\"Accuracy of Random Forest Classifier is :\",rfc.score(feat_test,np.argmax(y_test,axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exLFosgEKE_u"
      },
      "source": [
        "**4. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NVVMx3nEhq1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "y_pred=logreg.predict(feat_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import precision_score\n",
        "print('Precision: %.3f' % precision_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import recall_score\n",
        "print('Recall: %.3f' % recall_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import f1_score\n",
        "print('F1 Score: %.3f' % f1_score(np.argmax(y_test,axis=1), y_pred))\n",
        "#print(\"The Accuracy of Logistic Regression is :\",logreg.score(feat_test,np.argmax(y_test,axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXY__wtKNq_"
      },
      "source": [
        "**5. Naive Bayes Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYS7wR9OEljE"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB(priors=None, var_smoothing=1e-09)\n",
        "model.fit(feat_train,np.argmax(y_train,axis=1))\n",
        "y_pred=model.predict(feat_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: %.3f' % accuracy_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import precision_score\n",
        "print('Precision: %.3f' % precision_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import recall_score\n",
        "print('Recall: %.3f' % recall_score(np.argmax(y_test,axis=1), y_pred))\n",
        "from sklearn.metrics import f1_score\n",
        "print('F1 Score: %.3f' % f1_score(np.argmax(y_test,axis=1), y_pred))\n",
        "\n",
        "print(\"The Accuracy of Naive Bayes is :\",model.score(feat_test,np.argmax(y_test,axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veegFI2FKThj"
      },
      "source": [
        "**6. Proposed Methodology - CNN model(Framework - Densenet201)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1YwwOzNEnQ1"
      },
      "source": [
        "def build_model(backbone, lr=1e-4):\n",
        "    model = Sequential()\n",
        "    model.add(backbone)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(2, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(lr=lr),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "resnet = DenseNet201(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(120,120,3)\n",
        ")\n",
        "model = build_model(resnet ,lr = 1e-4)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brdq5ih5Yezo"
      },
      "source": [
        "***Training and Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zlSSm9CTEp20"
      },
      "source": [
        "learn_control = ReduceLROnPlateau(monitor='val_accuracy', patience=5,\n",
        "                                  verbose=1,factor=0.2, min_lr=1e-7)\n",
        "\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
        "    epochs=15,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[learn_control, checkpoint]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZn_7BSpYnBi"
      },
      "source": [
        "***Plotting the Graph***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMrBTSRxES9b"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['loss', 'val_loss']].plot()\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['accuracy', 'val_accuracy']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhefcG6y2LpH"
      },
      "source": [
        "history_df.to_csv('/content/drive/MyDrive/10epoch.csv', mode='a',header='f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uirYCXSYtE3J"
      },
      "source": [
        "***Prediction***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1SnxInLEuoo"
      },
      "source": [
        "#Load the model\n",
        "model.load_weights(\"weights.best.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuVt58R4EwQ4"
      },
      "source": [
        "Y_test_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyEp_QC6EyFW"
      },
      "source": [
        "print(\"The Acuuracy of CNN is :\",accuracy_score(np.argmax(y_test, axis=1), np.argmax(Y_test_pred, axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQsikCZ2E0Ae"
      },
      "source": [
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN04ZiRyE19W"
      },
      "source": [
        "tta_steps = 10\n",
        "predictions = []\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    preds = model.predict_generator(train_generator.flow(X_test, batch_size=BATCH_SIZE, shuffle=False),\n",
        "                                    steps = len(X_test)/BATCH_SIZE)\n",
        "    \n",
        "    predictions.append(preds)\n",
        "    gc.collect()\n",
        "    \n",
        "Y_pred_tta = np.mean(predictions, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxfGo3Tqs0m4"
      },
      "source": [
        "***Classification Report***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAsOzNdyE-Wd"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report( np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztpnrMOnsgJP"
      },
      "source": [
        "\n",
        "i=0\n",
        "prop_class=[]\n",
        "mis_class=[]\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "    if(np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n",
        "        prop_class.append(i)\n",
        "    if(len(prop_class)==8):\n",
        "        break\n",
        "\n",
        "i=0\n",
        "for i in range(len(Y_test)):\n",
        "    if(not np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n",
        "        mis_class.append(i)\n",
        "    if(len(mis_class)==8):\n",
        "        break\n",
        "\n",
        "# # Display first 8 images of benign\n",
        "w=60\n",
        "h=40\n",
        "fig=plt.figure(figsize=(18, 10))\n",
        "columns = 4\n",
        "rows = 2\n",
        "\n",
        "def Transfername(namecode):\n",
        "    if namecode==0:\n",
        "        return \"Benign\"\n",
        "    else:\n",
        "        return \"Malignant\"\n",
        "    \n",
        "for i in range(len(prop_class)):\n",
        "    ax = fig.add_subplot(rows, columns, i+1)\n",
        "    ax.set_title(\"Predicted result:\"+ Transfername(np.argmax(Y_pred_tta[prop_class[i]]))\n",
        "                       +\"\\n\"+\"Actual result: \"+ Transfername(np.argmax(Y_test[prop_class[i]])))\n",
        "    plt.imshow(X_test[prop_class[i]], interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_5OcUt3eVvu"
      },
      "source": [
        "**Save the complete model and re-train it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvc9K16NgH8h"
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB3g6N4eyUia"
      },
      "source": [
        "model = tf.keras.models.load_model('my_model.h5')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yctwAGckxQK"
      },
      "source": [
        "new_model.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CfH4EA1_k1de"
      },
      "source": [
        "\n",
        "history = new_model.fit_generator(\n",
        "    train_generator.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
        "    epochs=3,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[learn_control, checkpoint]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}